{% extends "base.html" %} {% block head %}
<link
  href="https://fonts.googleapis.com/icon?family=Material+Icons"
  rel="stylesheet"
/>
<link
  type="text/css"
  rel="stylesheet"
  href="{{ url_for('static', filename = 'css/maxcut.css') }}"
/>
<link
  rel="stylesheet"
  href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script
  type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
></script>
<style>
  .slide {
    display: none;
    width: 100%;
  }

  .slide.active {
    display: block;
  }

  .A {
    background-color: magenta;
  }


  .B {
    background-color: lightgreen;
  }

  .slide > img {
    width: 100%;
    height: auto;
  }

  .control-btn-group {
    display: flex;
    justify-content: space-evenly;
    width: 100vw;
    position: fixed;
    bottom: 0.2rem;
    margin-bottom: 2rem;
  }
  .control-btn-group > button {
    min-width: 20vw;
  }
</style>
{% endblock %} {% block body %}
<div class="container">
  <div class="row">
    <div class="col s12 m4 l8">
      <div class="h1">
        $\alpha$-approximation and $0.5$-approximation for maximization problems
      </div>
      <span class="content">
        TLDR: $\alpha$ is a ratio between the result obtained by the algorithm
        and the optimal result. $0.5$ means the result that an algo gives is
        half as good as the optimal result. Here is a formal definition: Let
        $\Pi$ be a maximization problem, and $\alpha \leq 1$ a constant. An
        algorithm $A$ is an $\alpha$-approximation algorithm for $\Pi$ if: 1. A
        runs in polynomial time. 2. For every input $I$ of $\Pi$ $$
        \operatorname{cost}(A(I)) \geq \alpha \times \operatorname{cost}(O P
        T(I)) . $$ $\alpha$ is called the "approximation ratio (or approximation
        factor)" of the algorithm $\Pi$.
      </span>
      <div class="h2">A Weighted Max-Cut problem</div>
      <span class="content">
        <b>Input</b>: An undirected weighted graph $G=(V, E)$.
        <br />
        <b>Output</b>: A partition of $V$ into two subsets $A, B$, where $B =
        \overline{A}$ such that the sum of weights of edges between $A$ and $B$
        is maximized.
        <br />
        <b>Objective Function</b>: Cut weights (or, sum of edge weights between
        $A$ and $B$).
      </span>

      <div class="h2">A naive randomized approach</div>
      <span class="content">
        A naive randomized approach is to scan through the vertex set in
        arbitary order, then put each vertex into $A$ or $B$ uniformly at
        random, i.e., the probability that a vertex falls in either $A$ or $B$
        is $\frac{1}{2}$. The random decisions for all vertices are independent.
        This algorithm is extremely simple. Let $E(A, B)$ denote the set of
        edges that have one endpoint in $A$, and the other is in $B$, like
        bridges connecting the two sets. The expected weight of the resulting
        cut is, by linearity of expectation, $$ \begin{aligned}
        &\mathbb{E}\left[\sum_{e \in E(A, B)} w_e\right] \\=&\sum_{e \in E(A,
        B)} \mathbb{E} [w_e] \\=&\sum_{e \in E} w_e \cdot \operatorname{Pr}(e
        \in E(A, B)) \\=&\frac{1}{2} \sum_{e \in E} w_e \end{aligned} $$ Observe
        that the upper bound ($OPT$, maximum cut) occurs when all edges are in
        $E(A,B)$. Then the sum of weights of $E(A,B)$ is the sum of weights of
        all edges. The expectation calculations shows that the weight of the cut
        generated by our randomized algorithm is at least *in expectation* half
        the weight of the maximum cut. In other words, among all the possible
        cuts (as much as $2^{|E|}$ cuts), at least one cut results in the cut
        weight less than half of the maximum weight. Can we improve? Introducing
        derandomization technique using conditional expectations, we can
        guarantee the output cut will be at least, not in expectation, greater
        than or equal to half of the maximum weight.</span
      >

      <div class="h2">Derandomization using conditional expectations</div>
      <div class="h3">Intuition</div>
      <span class="content">
        Instead of making all our random decisions at once, we make them in a
        sequential manner. In this technique with max-cut, we do not choose
        randomly between two options (in $A$ or in $B$). Instead, we assess both
        options based on the expected outcome of the objective function if we
        were to fix the decision and all previous ones, but make the remaining
        decisions randomly. Finally, we select the option that maximizes this
        expected outcome.
      </span>

      <div class="h3">Algorithm</div>
      <span class="content"
        >As we iterate, vertices can be categorized into three sets: vertices in
        $A$, vertices in $B$ and unassigned in $C$. At the beginning $A$ and $B$
        are empty. At termination, $C$ is empty; $A$ and $B$ partition the
        vertices. The expected weights of the random cut procuded by this
        algorithm are $$ w(A, B, C)=\sum_{e \in E(A, B)} w_e+\frac{1}{2} \sum_{e
        \in E(A, C)} w_e+\frac{1}{2} \sum_{e \in E(B, C)} w_e+\frac{1}{2}
        \sum_{e \in E(C, C)} w_e . $$ At the time a vertex $v$ is being
        inspected, we want to ask $$w(A+v, B, C-v)>w(A, B+v, C-v) \text{ }?$$
        Or, to write the expected weights $w(...)$ in summation form using
        linearity of expectation and then cancel terms, we are asking $$
        \frac{1}{2} \sum_{e \in E(B, v)} w_e>\frac{1}{2} \sum_{e \in E(A, v)}
        w_e \text{ }? $$ If true, then putting $v$ into $A$ is maximizing the
        current cut. If not, then putting into $B$ is more optimal.</span
      >

      <div class="h3">Motivation</div>
      <span class="content">Hello $Latex</span>

      <br />
      <span class="content">
        $$ \begin{aligned} & \textbf{ Algorithm }\text { Derandomized max-cut
        algorithm using method of conditional expectations } \\ & \hline \text {
        1: Initialize } A=B=\emptyset . \\ & \text { 2: for all } v \in V \text
        { do } \\ & \text { 3: if } \sum_{e \in E(B, v)} w_e-\sum_{e \in E(A,
        v)} w_e>0 \text { then } \\ & \text { 4: } \quad A=A+v \\ & 5: \quad
        \text { else } \\ & \text { 6: } \quad B=B+v \\ & \text { 7: end if } \\
        & \text { 8: end for } \\ & \text { 9: return } A, B & \\ & \hline
        \end{aligned} $$
      </span>

      <div class="h3">Analysis</div>
      <span class="content">
        Why this derandomization guarantees that the output cut is greater than
        or equal to half of the maximum weight? Observe that for every
        currently-inspected $v$ lies in one of $A$, $B$, $C$, we have $$
        \frac{1}{2} w(A+v, B, C-v)+\frac{1}{2} w(A, B+v, C-v)=w(A, B, C) $$
        Consequently $$ \max \{w(A+v, B, C-v), w(A, B+v, C-v)\} \geq w(A, B, C)
        $$ so the value of $w(A, B, C)$ never decreases till the termination.
        This deterministic algorithm runs in $O(|V|)$: the amount of time spent
        on the loop iteration that processes vertex $v$ is proportional to the
        length of the adjacency list of that vertex. A side note is that, if the
        adjacency matrix is given, then we dont need to spend $O(|E|)$ of time
        to build that.
      </span>

      <div class="h3">
        What does it mean by "it's guaranteed to succeed with a cut at least
        half the maximum cut"?
      </div>
      <span class="content">
        "Guarantee" means, no matter what the input sequence is, as long as we
        follow the deterministic algorithm, we will always get a cut with weight
        at least half the maximum cut. At the end of example, I made an
        experiment with ALL permutations of an input sequence. Each permutation
        succeeds with at least half of the maximum cut (ratio â‰¥ 0.5).
      </span>

      <div class="h3">Experiment</div>
      <span class="content">
        An example of the deterministic algorithm applied to a graph is shown
        below. At each step, I show that the invariant always holds. At the end
        I show no matter what input sequence of the vertex set is, the invariant
        for the final result always holds.
      </span>
      <span class="content">
        The example is based on a simple undirected weighted graph, with 9
        nodes, 15 edges, all weights positive, and sum of all weights 94.
        Weights are shown in <span style="background-color:yellow">yellow</span>. The vertices that have not been visited are
        <span style="background-color:gray">gray</span>. Current visiting vertex is circled in <span style="background-color:orange">orange</span>.
        <br />
        Before scanning the vertex set, we initalize two empty vertex sets, $A$
        and $B$.
        <br />
        Consider an arbitrary input sequence of the vertices:
        $[2,6,3,7,1,8,4,5,9]$
      </span>
      
      <hr>

      {%- for slide in slides -%}
      <div id="{{loop.index0}}" class="slide">
        <h4>Step {{loop.index}}/{{slides|length}}</h4>
        <div class="content">
            {{ slide["description"]|safe }}
        </div>
        <!--<img src="{{ slide["annotation"] }}" />-->
        <img src="{{ slide["img"] }}" />
      </div>
      {%- endfor -%}


<div class="h3">
  Does the example reveals the output invariant: it's guaranteed that the output cut is at least half as the maximum cut?
</div>
<span class="content"> 
  Yes, by running the algorithm for every permutation of the input sequence ($|V|!=362800$ runs), all ($362800/362800=1.0$) the cut weights surpass half ($0.5$ ratio) the total weights.
  <br>
  A code snippet for a glance:<br>
  <pre>
  V=[2,6,3,7,1,8,4,5,9] 
  perms = list(map(lambda pV : list(pV), set(permutations(V))))
  holds=0
  violatedPerm=[]
  for pV in tqdm(perms):
    ABweights, ratio, AAweights, BBweights=algoPrint(pV,E, draw=False)
    ratios.append(ratio)
    if ABweights < AAweights + BBweights:
      violatedPerm.append(pV)
    holds+=int(ABweights >= AAweights + BBweights)
  print(f"Number of cut that surpass half maximum cut: {holds}/{len(ratios)}={holds/len(ratios)}")
  </pre><br>
  And the output is <br>
  <pre>
  Number of cut that surpass half maximum cut: 362800/362800=1.0
  </pre><br>
  </span>


  <div class="h3">
    Interesting observation
  </div>
  <span class="content"> 
    Recall the implemental steps of our algorithm: we append the current vertex to $A$ or $B$ so as to maximize the current cut value (sum of edge weights between $A$ and $B$). This scheeme is just GREEDY algorithm!
    More generally, the greedy algorithm might be better than the expectation when all choices are random. </span>

    <hr>
<div class="h3">
  Citation
</div>
<span class="content">
  [Textbook] Mitzenmacher, Michael; Upfal, Eli (2005), Probability and Computing: Randomized Algorithms and Probabilistic Analysis, Cambridge.
  <br>
  [Lecture Notes] Vadhan, Salil (2009), Basic Derandomization Techniques, CS255 Pseudorandomness Lecture Notes, Harvard Univesity, Retrieved from https://people.seas.harvard.edu/~salil/cs225/spring09/lecnotes/Chap3.pdf
  <br>
  [Wiki] Maximum cut, Wikipedia, Retrieved from https://en.wikipedia.org/wiki/Maximum_cut
  <br>
  [Lecture Notes] Vadhan, Salil (2009), Basic Derandomization Techniques,CS255 Pseudorandomness Lecture Notes, Harvard Univesity, https://people.seas.harvard.edu/~salil/cs225/spring09/lecnotes/Chap3.pdf
  <br>
  [Textbook] Motwani, R., & Raghavan, P. (1995). The Probabilistic Method. In Randomized Algorithms (pp. 101â€“126). chapter 5, Cambridge: Cambridge University Press. http://doi.org/10.1017/CBO9780511814075.006
  <br>
  [Lecture Notes] Kleinberg, Bobby (2010). Randomized Approximation Algorithms, Lecture notes on approximation algorithms, Fall 2010, CS6820: Algorithms, Cornell University, November 10â€“15, 2010. Retrieved from https://www.cs.cornell.edu/courses/cs6820/2010fa/handouts/approx_algs.pdf
   </span>
    </div>
  </div>
  <br />
  <br />
  
</div>

<div class="control-btn-group">
  <button class="btn btn-primary btn-lg" onclick="to(-1)">prev</button>
  <button class="btn btn-primary btn-lg" onclick="to(1)">next</button>
</div>

<script>
  let currentIdx = 0;
  const totalLength = {{slides|length}};

  document.getElementById(currentIdx).classList.add("active");
  function to(step) {
    document.getElementById(currentIdx).classList.remove("active");
    currentIdx = (currentIdx + step) % totalLength;
    console.log(currentIdx);
    document.getElementById(currentIdx).classList.add("active");
  }
</script>

{% endblock %}
